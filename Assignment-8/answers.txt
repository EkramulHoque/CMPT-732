1.What did you see in the execution plan for the “join in Spark” solution? Why was the execution so fast (and the memory usage so small)?

Spark has 'push down' filter which allows the database for better optimized queries. Here we are filtering records according to
'orderkeys' which has reduced the number of entries retrieved from the database and improve query performance. And when the 
'broadcasthashjoin' is exectuted, it performs better with small entries i.e small memory usage making the execution faster.

== Physical Plan ==
ObjectHashAggregate(keys=[orderkey#0, totalprice#8], functions=[collect_set(name#56, 0, 0)])
+- Exchange hashpartitioning(orderkey#0, totalprice#8, 200)
   +- ObjectHashAggregate(keys=[orderkey#0, totalprice#8], functions=[partial_collect_set(name#56, 0, 0)])
      +- *(3) Project [orderkey#0, totalprice#8, name#56]
         +- *(3) BroadcastHashJoin [partkey#26], [partkey#51], Inner, BuildRight
            :- *(3) Project [orderkey#0, totalprice#8, partkey#26]
            :  +- *(3) BroadcastHashJoin [orderkey#0], [orderkey#20], Inner, BuildLeft
            :     :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)))
            :     :  +- *(1) Filter isnotnull(orderkey#0)
            :     :     +- *(1) Scan org.apache.spark.sql.cassandra.CassandraSourceRelation@63f1bcf7 [orderkey#0,totalprice#8] PushedFilters: [*In(orderkey, [151201,986499,28710,193734,810689]), IsNotNull(orderkey)], ReadSchema: struct<orderkey:int,totalprice:decimal(38,18)>
            :     +- *(3) Filter (isnotnull(orderkey#20) && isnotnull(partkey#26))
            :        +- *(3) Scan org.apache.spark.sql.cassandra.CassandraSourceRelation@1a94172a [orderkey#20,partkey#26] PushedFilters: [IsNotNull(orderkey), *In(orderkey, [151201,986499,28710,193734,810689]), IsNotNull(partkey)], ReadSchema: struct<orderkey:int,partkey:int>
            +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)))
               +- *(2) Filter isnotnull(partkey#51)
                  +- *(2) Scan org.apache.spark.sql.cassandra.CassandraSourceRelation@9c15426 [partkey#51,name#56] PushedFilters: [IsNotNull(partkey)], ReadSchema: struct<partkey:int,name:string>


2.What was the CREATE TABLE statement you used for the orders_parts table?

CREATE TABLE ehoque.orders_parts (
    orderkey int PRIMARY KEY,
    clerk text,
    comment text,
    custkey int,
    order_priority text,
    orderdate date,
    orderstatus text,
    ship_priority int,
    part_names set<text>,	
    totalprice decimal
) WITH bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
    AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99PERCENTILE';

3.What were the running times of the two tpch_orders_* programs on the tpch2 data on the cluster? These orderkeys have results in that 
data set: 2579142 2816486 586119 441985 2863331.

tpch_orders_df.py
real    1m23.171s
user    0m47.884s
sys     0m3.092s

tpch_orders_denorm.py
real    0m35.339s
user    0m17.700s
sys     0m1.348s


4.Consider the logic that you would have to implement to maintain the denormalized data (assuming that the orders table had the part_names 
column in the main data set). Write a few sentences on what you'd have to do when inserting/updating/deleting data in this case.

We will be assuiming each entry in order table will have some part referenced from part table linked through lineitem table. Also update, 
insert and delete operation will happen in denormalized order table and we will update the other two table accordingly.Here will be the 
different scenarios

Insert: If data is inserted in 'order' table it should be reflected in 'lineitem' table for each 'part_names' created. 

Update: Similar procedure like insertion. If we update on 'part_names' table, it should be updated in the 'lineitem' table. The remaining fields will be unaffected.

Deletion: For instance a row is deleted in 'order' table, then all the entries in the 'lineitem' table will be deleted according to the 'part_names' created. But in part
table, it will be only deleted if the rows with 'part_names' reference is not linked with other 'lineitem' entries

The data must be consistent among the reference tables for all the three operations 
