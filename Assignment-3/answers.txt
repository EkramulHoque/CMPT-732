1.What was wrong with the original wordcount-5 data set that made repartitioning worth it? Why did the program run faster after?

Answer: The original wordcount-5 had a non-uniform data set.The repartition does a full shuffle of the data and creates equal sized partitions of data. When we had 8 executors, only 1 was working and others were being idle. Now, the program run faster since all our nodes/threads have the same amount of work.


2.The same fix does not make this code run faster on the wordcount-3 data set. (It may be slightly slower?) Why? [For once, the answer is not “the data set is too small”.]

Answer: The wordcount-3 data was not skewed and it was distributed uniformly.  


3.How could you modify the wordcount-5 input so that the word count code can process it and get the same results as fast as possible? (It's possible to get about another minute off the running time.)
Answers: We can manually split the wordcount-5 data set in equal filesizes. 

4.When experimenting with the number of partitions while estimating Euler's constant, you likely didn't see much difference for a range of values, and chose the final value in your code somewhere in that range. What range of partitions numbers was “good” (on the desktop/laptop where you were testing)?

Answers: Too few partitions will have enormous chunks of data, especially when we are dealing with bigdata, thus putting our application in memory stress. Too many partitions will have our hdfs taking much pressure, since all the metadata that has to be generated from the hdfs increases significantly as the number of partitions increase (since it maintains temp files, etc.).

The rule of thumb takes the number of cores into consideration i.e if the machine has 4 cores, the number of slices can be 4,8 or 12. In other words, it prescribes that one should have 1 partition per CPU core.

5.How much overhead does it seem like Spark adds to a job? How much speedup did PyPy get over the usual Python implementation? 
